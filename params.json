{"name":"PHP-Spider","tagline":"A configurable and extensible PHP web spider","body":"Welcome to PHP-Spider\r\n======\r\nA configurable and extensible PHP web spider. Full documentation will follow when all major refactoring is done. Until then, see below for a quickstart, or `example/example_complex.php` for a more thorough example.\r\n\r\nUsage\r\n-----\r\n\r\nThis is a very simple example. This code can be found in `example/example_simple.php`. For a more complete example with\r\nsome logging, caching and filters, see `example/example_complex.php`. That file contains a more real-world example.\r\n\r\nFirst create the spider\r\n```php\r\nuse VDB\\Spider\\Spider;\r\nuse VDB\\Spider\\Discoverer\\XPathExpressionDiscoverer;\r\n\r\n$spider = new Spider('http://www.dmoz.org');\r\n```\r\nAdd a URI discoverer. Without it, the spider does nothing. In this case, we want all `<a>` nodes from a certain `<div>`\r\n\r\n```php\r\n$spider->addDiscoverer(new XPathExpressionDiscoverer(\"//div[@id='catalogs']//a\"));\r\n```\r\nSet some sane options for this example. In this case, we only get the first 10 items from the start page.\r\n```php\r\n$spider->setMaxDepth(1);\r\n$spider->setMaxQueueSize(10);\r\n```\r\nExecute crawl\r\n```php\r\n$report = $spider->crawl();\r\n```\r\nAnd finally, we could get some info about the crawl\r\n```php\r\necho \"\\nENQUEUED: \" . count($report['queued']);\r\necho \"\\n - \".implode(\"\\n - \", $report['queued']);\r\necho \"\\nSKIPPED:   \" . count($report['filtered']);\r\necho \"\\nFAILED:    \" . count($report['failed']) . \"\\n\";\r\n```\r\n\r\nTODO\r\n----\r\n### MUST HAVE\r\n\r\n- [ ] refactor: Make the processqueue an injectable interface with some default adapters for file, memcache. etc.\r\n- [ ] refactor: make the returned report an injectable interface with some default adapters for file, memcache. Also contains info about where to find the process queue\r\n- [ ] refactor: make the spider accept an array of seeds\r\n- [ ] build: support robots.txt.\r\n- [ ] build: ranking policy listener. This can listen to the SPIDER_CRAWL_POST_DISCOVER event. We need to refactor $discoveredLinks to an object to let the listener change it. Question: Do we want to do that without copying the array because of memory usage?\r\n- [ ] build: a re-visit policy that states when to check for changes to the pages (\r\n- [ ] check: when calculating maxdepth, are redirects counted?\r\n\r\n### SHOULD HAVE\r\n\r\n- [ ] design: Support RDF, RSS Atom, Twitter feeds\r\n- [ ] decide: maybe make it possible to set a minimum depth for a discoverer.  Use case: index pages where detail pages have other markup structure\r\n- [ ] build: support scaling: multiple parallel threads / workers. For requests? or for spiders themselves? or both?\r\n- [ ] build: support authentication\r\n- [ ] build: Phar compilation support","google":"UA-38994599-1","note":"Don't delete this file! It's used internally to help with page regeneration."}