{"name":"PHP-Spider","tagline":"A configurable and extensible PHP web spider","body":"PHP-Spider Features\r\n======\r\n- supports two traversal algorithms: breadth-first and depth-first\r\n- supports depth limiting and queue size limiting\r\n- supports adding custom discovery logic, based on XPath, CSS selectors, or custom logic\r\n- comes with a useful set of prebuilt filters\r\n- supports custom filters, both prefetch and postfetch\r\n- supports custom request handling\r\n- dispatches useful events, allowing developers to add even more custom behavior\r\n- supports a politeness policy\r\n- will soon come with many default discoverers: RSS, Atom, RDF, etc.\r\n- will soon support multiple queueing and persistence mechanisms (file, memcache, redis)\r\n- will eventually support distributed spidering with a central queue\r\n\r\nUsage\r\n-----\r\nThis is a very simple example. This code can be found in [example/example_simple.php](https://github.com/matthijsvandenbos/php-spider/blob/master/example/example_simple.php). For a more complete example with\r\nsome logging, caching and filters, see [example/example_complex.php](https://github.com/matthijsvandenbos/php-spider/blob/master/example/example_complex.php). That file contains a more real-world example.\r\n\r\nFirst create the spider\r\n```php\r\nuse VDB\\Spider\\Spider;\r\nuse VDB\\Spider\\Discoverer\\XPathExpressionDiscoverer;\r\n\r\n$spider = new Spider('http://www.dmoz.org');\r\n```\r\nAdd a URI discoverer. Without it, the spider does nothing. In this case, we want all `<a>` nodes from a certain `<div>`\r\n\r\n```php\r\n$spider->addDiscoverer(new XPathExpressionDiscoverer(\"//div[@id='catalogs']//a\"));\r\n```\r\nSet some sane options for this example. In this case, we only get the first 10 items from the start page.\r\n```php\r\n$spider->setMaxDepth(1);\r\n$spider->setMaxQueueSize(10);\r\n```\r\nExecute crawl\r\n```php\r\n$report = $spider->crawl();\r\n```\r\nAnd finally, we could get some info about the crawl\r\n```php\r\necho \"\\nENQUEUED: \" . count($report['queued']);\r\necho \"\\n - \".implode(\"\\n - \", $report['queued']);\r\necho \"\\nSKIPPED:   \" . count($report['filtered']);\r\necho \"\\nFAILED:    \" . count($report['failed']) . \"\\n\";\r\n```\r\n\r\nTODO\r\n----\r\n### MUST HAVE\r\n\r\n- [ ] refactor: Make the processqueue an injectable interface with some default adapters for file, memcache. etc.\r\n- [ ] refactor: make the returned report an injectable interface with some default adapters for file, memcache. Also contains info about where to find the process queue\r\n- [ ] refactor: make the spider accept an array of seeds\r\n- [ ] build: support robots.txt.\r\n- [ ] build: ranking policy listener. This can listen to the SPIDER_CRAWL_POST_DISCOVER event. We need to refactor $discoveredLinks to an object to let the listener change it. Question: Do we want to do that without copying the array because of memory usage?\r\n- [ ] build: a re-visit policy that states when to check for changes to the pages (\r\n- [ ] check: when calculating maxdepth, are redirects counted?\r\n\r\n### SHOULD HAVE\r\n\r\n- [ ] design: Support RDF, RSS Atom, Twitter feeds\r\n- [ ] decide: maybe make it possible to set a minimum depth for a discoverer.  Use case: index pages where detail pages have other markup structure\r\n- [ ] build: support scaling: multiple parallel threads / workers. For requests? or for spiders themselves? or both?\r\n- [ ] build: support authentication\r\n- [ ] build: Phar compilation support\r\n","google":"UA-38994599-1","note":"Don't delete this file! It's used internally to help with page regeneration."}