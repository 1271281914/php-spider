<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="PHP-Spider : A configurable and extensible PHP web spider" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>PHP-Spider</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/matthijsvandenbos/php-spider">View on GitHub</a>

          <h1 id="project_title">PHP-Spider</h1>
          <h2 id="project_tagline">A configurable and extensible PHP web spider</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/matthijsvandenbos/php-spider/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/matthijsvandenbos/php-spider/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>Welcome to PHP-Spider</h1>

<p>A configurable and extensible PHP web spider. Full documentation will follow when all major refactoring is done. Until then, see below for a quickstart, or <code>example/example_complex.php</code> for a more thorough example.</p>

<h2>Usage</h2>

<p>This is a very simple example. This code can be found in <code>example/example_simple.php</code>. For a more complete example with
some logging, caching and filters, see <code>example/example_complex.php</code>. That file contains a more real-world example.</p>

<p>First create the spider</p>

<div class="highlight"><pre><span class="k">use</span> <span class="nx">VDB\Spider\Spider</span><span class="p">;</span>
<span class="k">use</span> <span class="nx">VDB\Spider\Discoverer\XPathExpressionDiscoverer</span><span class="p">;</span>

<span class="nv">$spider</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">Spider</span><span class="p">(</span><span class="s1">'http://www.dmoz.org'</span><span class="p">);</span>
</pre></div>

<p>Add a URI discoverer. Without it, the spider does nothing. In this case, we want all <code>&lt;a&gt;</code> nodes from a certain <code>&lt;div&gt;</code></p>

<div class="highlight"><pre><span class="nv">$spider</span><span class="o">-&gt;</span><span class="na">addDiscoverer</span><span class="p">(</span><span class="k">new</span> <span class="nx">XPathExpressionDiscoverer</span><span class="p">(</span><span class="s2">"//div[@id='catalogs']//a"</span><span class="p">));</span>
</pre></div>

<p>Set some sane options for this example. In this case, we only get the first 10 items from the start page.</p>

<div class="highlight"><pre><span class="nv">$spider</span><span class="o">-&gt;</span><span class="na">setMaxDepth</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="nv">$spider</span><span class="o">-&gt;</span><span class="na">setMaxQueueSize</span><span class="p">(</span><span class="mi">10</span><span class="p">);</span>
</pre></div>

<p>Execute crawl</p>

<div class="highlight"><pre><span class="nv">$report</span> <span class="o">=</span> <span class="nv">$spider</span><span class="o">-&gt;</span><span class="na">crawl</span><span class="p">();</span>
</pre></div>

<p>And finally, we could get some info about the crawl</p>

<div class="highlight"><pre><span class="k">echo</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">ENQUEUED: "</span> <span class="o">.</span> <span class="nb">count</span><span class="p">(</span><span class="nv">$report</span><span class="p">[</span><span class="s1">'queued'</span><span class="p">]);</span>
<span class="k">echo</span> <span class="s2">"</span><span class="se">\n</span><span class="s2"> - "</span><span class="o">.</span><span class="nb">implode</span><span class="p">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> - "</span><span class="p">,</span> <span class="nv">$report</span><span class="p">[</span><span class="s1">'queued'</span><span class="p">]);</span>
<span class="k">echo</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">SKIPPED:   "</span> <span class="o">.</span> <span class="nb">count</span><span class="p">(</span><span class="nv">$report</span><span class="p">[</span><span class="s1">'filtered'</span><span class="p">]);</span>
<span class="k">echo</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">FAILED:    "</span> <span class="o">.</span> <span class="nb">count</span><span class="p">(</span><span class="nv">$report</span><span class="p">[</span><span class="s1">'failed'</span><span class="p">])</span> <span class="o">.</span> <span class="s2">"</span><span class="se">\n</span><span class="s2">"</span><span class="p">;</span>
</pre></div>

<h2>TODO</h2>

<h3>MUST HAVE</h3>

<ul>
<li>[ ] refactor: Make the processqueue an injectable interface with some default adapters for file, memcache. etc.</li>
<li>[ ] refactor: make the returned report an injectable interface with some default adapters for file, memcache. Also contains info about where to find the process queue</li>
<li>[ ] refactor: make the spider accept an array of seeds</li>
<li>[ ] build: support robots.txt.</li>
<li>[ ] build: ranking policy listener. This can listen to the SPIDER_CRAWL_POST_DISCOVER event. We need to refactor $discoveredLinks to an object to let the listener change it. Question: Do we want to do that without copying the array because of memory usage?</li>
<li>[ ] build: a re-visit policy that states when to check for changes to the pages (</li>
<li>[ ] check: when calculating maxdepth, are redirects counted?</li>
</ul><h3>SHOULD HAVE</h3>

<ul>
<li>[ ] design: Support RDF, RSS Atom, Twitter feeds</li>
<li>[ ] decide: maybe make it possible to set a minimum depth for a discoverer.  Use case: index pages where detail pages have other markup structure</li>
<li>[ ] build: support scaling: multiple parallel threads / workers. For requests? or for spiders themselves? or both?</li>
<li>[ ] build: support authentication</li>
<li>[ ] build: Phar compilation support</li>
</ul>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">PHP-Spider maintained by <a href="https://github.com/matthijsvandenbos">matthijsvandenbos</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-38994599-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>


  </body>
</html>
